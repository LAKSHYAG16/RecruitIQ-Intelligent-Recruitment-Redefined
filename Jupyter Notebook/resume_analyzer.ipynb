{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f41d06fa-31b9-4345-a2b5-6558a934fe98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import fitz  # PyMuPDF\n",
    "import base64\n",
    "import streamlit as st\n",
    "import spacy\n",
    "import csv\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp_skills = spacy.load('TrainedModel/skills')  # Custom NER model for skills\n",
    "\n",
    "# Load keywords from CSV\n",
    "def load_keywords(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        return set(row[0] for row in reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3b336dd7-925e-4187-b0f6-b8f33f504ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Name\n",
    "\n",
    "def extract_name(text):\n",
    "    # Take top 10 lines from the text\n",
    "    lines = text.split('\\n')\n",
    "    top_lines = '\\n'.join(lines[:10])\n",
    "\n",
    "    # Run spaCy on top lines\n",
    "    top_doc = nlp(top_lines)\n",
    "\n",
    "    # List of keywords to avoid (to prevent false positives like \"Query Rewrite\")\n",
    "    bad_keywords = ['query', 'rewrite', 'data', 'sql', 'analytics', 'project', 'objective']\n",
    "\n",
    "    for ent in top_doc.ents:\n",
    "        if ent.label_ == 'PERSON':\n",
    "            name_candidate = ent.text.strip()\n",
    "            if not any(word.lower() in bad_keywords for word in name_candidate.split()):\n",
    "                names = name_candidate.split()\n",
    "                if len(names) >= 2 and all(name[0].isupper() for name in names):\n",
    "                    return names[0], ' '.join(names[1:])\n",
    "    return \"\", \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d6aca6fc-0b0b-4e07-93c6-c6f96c3f9879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Email\n",
    "\n",
    "# Extract Email\n",
    "def extract_email(text):\n",
    "    email_match = re.search(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', text)\n",
    "    return email_match.group() if email_match else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f360cced-decf-4b20-882c-489b1340c5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Ph No\n",
    "\n",
    "# Extract Contact Number\n",
    "def extract_contact_number(text):\n",
    "    pattern = r\"\\b(?:\\+?\\d{1,3}[-.\\s]?)?\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b\"\n",
    "    match = re.search(pattern, text)\n",
    "    return match.group() if match else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e85cfe48-4836-413e-b85e-7cef0c7312b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract Education\n",
    "\n",
    "# Extract Education\n",
    "def extract_education(text):\n",
    "    doc = nlp(text)\n",
    "    universities = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"ORG\" and any(x in ent.text.lower() for x in [\"university\", \"college\", \"institute\"]):\n",
    "            universities.append(ent.text)\n",
    "    return universities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "297661d4-88d8-4538-8843-fcc350934e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Skills\n",
    "\n",
    "# CSV-based Skills Extraction\n",
    "def csv_skills(text):\n",
    "    skills_keywords = load_keywords('newSkills.csv')\n",
    "    return {kw for kw in skills_keywords if kw.lower() in text.lower()}\n",
    "\n",
    "# NER-based Skills Extraction\n",
    "def extract_skills_from_ner(text):\n",
    "    non_skill_labels = {'DATE', 'TIME', 'PERCENT', 'MONEY', 'QUANTITY', 'ORDINAL', 'CARDINAL', 'EMAIL'}\n",
    "    doc = nlp_skills(text)\n",
    "    return {ent.text.strip() for ent in doc.ents if ent.label_ == 'SKILL' and ent.label_ not in non_skill_labels}\n",
    "\n",
    "# Combine and Clean Skills\n",
    "def extract_skills(text):\n",
    "    skills = csv_skills(text).union(extract_skills_from_ner(text))\n",
    "    return [s for s in skills if s and s.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dc16a292-c9d2-4a6e-a541-199815abd8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract Major\n",
    "# Major/Degree Extraction\n",
    "def extract_major(text):\n",
    "    majors = load_keywords('majors.csv')\n",
    "    for keyword in majors:\n",
    "        if keyword.lower() in text.lower():\n",
    "            return keyword\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9dfdd8b3-2418-4991-9612-bf14b0408b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract Experience\n",
    "# Experience Extraction\n",
    "def extract_experience(text):\n",
    "    doc = nlp(text)\n",
    "    verbs = [token.lemma_.lower() for token in doc if token.pos_ == 'VERB']\n",
    "    senior = ['lead', 'manage', 'direct', 'oversee']\n",
    "    mid = ['develop', 'design', 'analyze', 'implement']\n",
    "    junior = ['assist', 'support', 'contribute']\n",
    "\n",
    "    if any(v in verbs for v in senior):\n",
    "        level = \"Senior\"\n",
    "    elif any(v in verbs for v in mid):\n",
    "        level = \"Mid-Senior\"\n",
    "    elif any(v in verbs for v in junior):\n",
    "        level = \"Mid-Junior\"\n",
    "    else:\n",
    "        level = \"Entry Level\"\n",
    "\n",
    "    # position = suggest_position(verbs)\n",
    "    return {'level_of_experience': level}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "78f275b8-5203-4b12-ad24-2763c6483a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:14: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:16: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:17: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:18: SyntaxWarning: invalid escape sequence '\\]'\n",
      "<>:20: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:14: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:16: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:17: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:18: SyntaxWarning: invalid escape sequence '\\]'\n",
      "<>:20: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_12916\\1467648555.py:14: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  cleanText = re.sub('http\\S+\\s', ' ', txt)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_12916\\1467648555.py:16: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  cleanText = re.sub('#\\S+\\s', ' ', cleanText)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_12916\\1467648555.py:17: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  cleanText = re.sub('@\\S+', '  ', cleanText)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_12916\\1467648555.py:18: SyntaxWarning: invalid escape sequence '\\]'\n",
      "  cleanText = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), ' ', cleanText)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_12916\\1467648555.py:20: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  cleanText = re.sub('\\s+', ' ', cleanText)\n"
     ]
    }
   ],
   "source": [
    "# # Suggest Position\n",
    "\n",
    "# Required imports\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "# Load TF-IDF, classifier, and encoder\n",
    "tfidf = pickle.load(open('tfidf.pkl', 'rb'))\n",
    "svc_model = pickle.load(open('clf.pkl', 'rb'))\n",
    "le = pickle.load(open('encoder.pkl', 'rb'))\n",
    "\n",
    "# Clean the resume text\n",
    "def cleanResume(txt):\n",
    "    cleanText = re.sub('http\\S+\\s', ' ', txt)\n",
    "    cleanText = re.sub('RT|cc', ' ', cleanText)\n",
    "    cleanText = re.sub('#\\S+\\s', ' ', cleanText)\n",
    "    cleanText = re.sub('@\\S+', '  ', cleanText)  \n",
    "    cleanText = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), ' ', cleanText)\n",
    "    cleanText = re.sub(r'[^\\x00-\\x7f]', ' ', cleanText) \n",
    "    cleanText = re.sub('\\s+', ' ', cleanText)\n",
    "    return cleanText.strip()\n",
    "\n",
    "# Predict the position from resume text\n",
    "def predict_position_from_text(text):\n",
    "    cleaned_text = cleanResume(text)\n",
    "    vectorized = tfidf.transform([cleaned_text]).toarray()\n",
    "    prediction = svc_model.predict(vectorized)\n",
    "    return le.inverse_transform(prediction)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f2f14ea3-9a00-4b2f-8e25-b9a3ca302548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_resume_score(resume_info):\n",
    "# Resume Scoring\n",
    "def calculate_resume_score(info):\n",
    "    score = 0\n",
    "    score += 25 if info['first_name'] and info['last_name'] else 0\n",
    "    score += 25 if info['email'] else 0\n",
    "    score += 25 if info['degree_major'] else 0\n",
    "    score += 25 if info['skills'] else 0\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4abf0b84-383d-4fb3-b70e-3600fb2dc864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suggested Skills for Job Title\n",
    "def suggest_skills_for_job(job):\n",
    "    job_skills = {}\n",
    "    with open('sugestedSkills.csv') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            job_title = row[0].strip().lower()\n",
    "            skills = [s.strip() for s in row[1:] if s]\n",
    "            job_skills[job_title] = skills\n",
    "    return job_skills.get(job.lower(), [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3f40ceb1-bab7-474a-a62d-fd66713cedd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1613576-b70b-4876-80dc-ae9af7219215",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'extract_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([page\u001b[38;5;241m.\u001b[39mget_text() \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m doc])\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Extract all info\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m first_name, last_name \u001b[38;5;241m=\u001b[39m extract_name(text)\n\u001b[0;32m      9\u001b[0m email \u001b[38;5;241m=\u001b[39m extract_email(text)\n\u001b[0;32m     10\u001b[0m phone \u001b[38;5;241m=\u001b[39m extract_contact_number(text)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'extract_name' is not defined"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "# Load resume PDF and extract text\n",
    "doc = fitz.open(r\"C:\\Users\\HP\\Desktop\\sample_of_a_resume4.pdf\")\n",
    "text = \"\\n\".join([page.get_text() for page in doc])\n",
    "\n",
    "# Extract all info\n",
    "first_name, last_name = extract_name(text)\n",
    "email = extract_email(text)\n",
    "phone = extract_contact_number(text)\n",
    "skills = extract_skills(text)\n",
    "degree_major = extract_major(text)\n",
    "experience = extract_experience(text)\n",
    "\n",
    "# Use ML model to predict position\n",
    "predicted_position = predict_position_from_text(text)\n",
    "\n",
    "resume_info = {\n",
    "    'first_name': first_name,\n",
    "    'last_name': last_name,\n",
    "    'email': email,\n",
    "    'phone': phone,\n",
    "    'skills': skills,\n",
    "    'degree_major': degree_major,\n",
    "    'experience': {\n",
    "        'level_of_experience': experience['level_of_experience'],\n",
    "        'suggested_position': predicted_position   # REPLACED OLD POSITION WITH ML ONE\n",
    "    }\n",
    "}\n",
    "\n",
    "# Display\n",
    "print(\"First Name:\", resume_info['first_name'])\n",
    "print(\"Last Name:\", resume_info['last_name'])\n",
    "print(\"Email:\", resume_info['email'])\n",
    "print(\"Phone:\", resume_info['phone'])\n",
    "print(\"Degree/Major:\", resume_info['degree_major'])\n",
    "print(\"Skills:\", ', '.join(resume_info['skills']))\n",
    "print(\"Experience Level:\", resume_info['experience']['level_of_experience'])\n",
    "print(\"Suggested Position:\", resume_info['experience']['suggested_position'])\n",
    "\n",
    "score = calculate_resume_score(resume_info)\n",
    "print(\"Resume Score:\", score)\n",
    "\n",
    "suggested_skills = suggest_skills_for_job(resume_info['experience']['suggested_position'])\n",
    "print(\"Suggested Skills for This Role:\", ', '.join(suggested_skills) if suggested_skills else \"None\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e08a84-a1dc-48f3-8fdb-4e2a69b0461b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e15d244-e7ce-4df5-856a-cfd7eb3b3246",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:131: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:133: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:134: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:135: SyntaxWarning: invalid escape sequence '\\]'\n",
      "<>:137: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:131: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:133: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:134: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:135: SyntaxWarning: invalid escape sequence '\\]'\n",
      "<>:137: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_7244\\821012915.py:131: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  cleanText = re.sub('http\\S+\\s', ' ', txt)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_7244\\821012915.py:133: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  cleanText = re.sub('#\\S+\\s', ' ', cleanText)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_7244\\821012915.py:134: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  cleanText = re.sub('@\\S+', '  ', cleanText)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_7244\\821012915.py:135: SyntaxWarning: invalid escape sequence '\\]'\n",
      "  cleanText = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), ' ', cleanText)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_7244\\821012915.py:137: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  cleanText = re.sub('\\s+', ' ', cleanText)\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Name: \n",
      "Last Name: \n",
      "Email: itsomsarraf@gmail.com\n",
      "Phone: 91 9868540784\n",
      "Degree/Major: \n",
      "Skills: TypeScript, Developing, Django, CFD, Intern, SQL, JavaScript, Java, Redis, PostgreSQL, multipage, GitHub, Lighthouse, MLH, OS, Git, June, Led, MongoDB, Hackathon, Lead, Go, React, Python, Docker, R, messages, Built\n",
      "Experience Level: Senior\n",
      "Suggested Position: Blockchain\n",
      "Resume Score: 50\n",
      "Suggested Skills for This Role: None\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import fitz  # PyMuPDF\n",
    "import base64\n",
    "import streamlit as st\n",
    "import spacy\n",
    "import csv\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp_skills = spacy.load('TrainedModel/skills')  # Custom NER model for skills\n",
    "# Load keywords from CSV\n",
    "def load_keywords(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        return set(row[0] for row in reader)\n",
    "\n",
    "\n",
    "\n",
    "# Extract Name\n",
    "def extract_name(text):\n",
    "    # Take top 10 lines from the text\n",
    "    lines = text.split('\\n')\n",
    "    top_lines = '\\n'.join(lines[:10])\n",
    "\n",
    "    # Run spaCy on top lines\n",
    "    top_doc = nlp(top_lines)\n",
    "\n",
    "    # List of keywords to avoid (to prevent false positives like \"Query Rewrite\")\n",
    "    bad_keywords = ['query', 'rewrite', 'data', 'sql', 'analytics', 'project', 'objective']\n",
    "\n",
    "    for ent in top_doc.ents:\n",
    "        if ent.label_ == 'PERSON':\n",
    "            name_candidate = ent.text.strip()\n",
    "            if not any(word.lower() in bad_keywords for word in name_candidate.split()):\n",
    "                names = name_candidate.split()\n",
    "                if len(names) >= 2 and all(name[0].isupper() for name in names):\n",
    "                    return names[0], ' '.join(names[1:])\n",
    "    return \"\", \"\"\n",
    "\n",
    "\n",
    "\n",
    "# Extract Email\n",
    "def extract_email(text):\n",
    "    email_match = re.search(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', text)\n",
    "    return email_match.group() if email_match else \"\"\n",
    "\n",
    "\n",
    "\n",
    "# Extract Contact Number\n",
    "def extract_contact_number(text):\n",
    "    pattern = r\"\\b(?:\\+?\\d{1,3}[-.\\s]?)?\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b\"\n",
    "    match = re.search(pattern, text)\n",
    "    return match.group() if match else \"\"\n",
    "\n",
    "\n",
    "\n",
    "# Extract Education\n",
    "def extract_education(text):\n",
    "    doc = nlp(text)\n",
    "    universities = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"ORG\" and any(x in ent.text.lower() for x in [\"university\", \"college\", \"institute\"]):\n",
    "            universities.append(ent.text)\n",
    "    return universities\n",
    "\n",
    "\n",
    "\n",
    "# Extract Skills\n",
    "# CSV-based Skills Extraction\n",
    "def csv_skills(text):\n",
    "    skills_keywords = load_keywords('newSkills.csv')\n",
    "    return {kw for kw in skills_keywords if kw.lower() in text.lower()}\n",
    "\n",
    "# NER-based Skills Extraction\n",
    "def extract_skills_from_ner(text):\n",
    "    non_skill_labels = {'DATE', 'TIME', 'PERCENT', 'MONEY', 'QUANTITY', 'ORDINAL', 'CARDINAL', 'EMAIL'}\n",
    "    doc = nlp_skills(text)\n",
    "    return {ent.text.strip() for ent in doc.ents if ent.label_ == 'SKILL' and ent.label_ not in non_skill_labels}\n",
    "\n",
    "# Combine and Clean Skills\n",
    "def extract_skills(text):\n",
    "    skills = csv_skills(text).union(extract_skills_from_ner(text))\n",
    "    return [s for s in skills if s and s.isalpha()]\n",
    "\n",
    "\n",
    "\n",
    "# Extract Major\n",
    "def extract_major(text):\n",
    "    majors = load_keywords('majors.csv')\n",
    "    for keyword in majors:\n",
    "        if keyword.lower() in text.lower():\n",
    "            return keyword\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "\n",
    "# Extract Experience\n",
    "def extract_experience(text):\n",
    "    doc = nlp(text)\n",
    "    verbs = [token.lemma_.lower() for token in doc if token.pos_ == 'VERB']\n",
    "    senior = ['lead', 'manage', 'direct', 'oversee']\n",
    "    mid = ['develop', 'design', 'analyze', 'implement']\n",
    "    junior = ['assist', 'support', 'contribute']\n",
    "\n",
    "    if any(v in verbs for v in senior):\n",
    "        level = \"Senior\"\n",
    "    elif any(v in verbs for v in mid):\n",
    "        level = \"Mid-Senior\"\n",
    "    elif any(v in verbs for v in junior):\n",
    "        level = \"Mid-Junior\"\n",
    "    else:\n",
    "        level = \"Entry Level\"\n",
    "\n",
    "    # position = suggest_position(verbs)\n",
    "    return {'level_of_experience': level}\n",
    "\n",
    "\n",
    "\n",
    "# Suggest Position\n",
    "# Required imports\n",
    "import pickle\n",
    "import re\n",
    "# Load TF-IDF, classifier, and encoder\n",
    "tfidf = pickle.load(open('tfidf.pkl', 'rb'))\n",
    "svc_model = pickle.load(open('clf.pkl', 'rb'))\n",
    "le = pickle.load(open('encoder.pkl', 'rb'))\n",
    "\n",
    "# Clean the resume text\n",
    "def cleanResume(txt):\n",
    "    cleanText = re.sub('http\\S+\\s', ' ', txt)\n",
    "    cleanText = re.sub('RT|cc', ' ', cleanText)\n",
    "    cleanText = re.sub('#\\S+\\s', ' ', cleanText)\n",
    "    cleanText = re.sub('@\\S+', '  ', cleanText)  \n",
    "    cleanText = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), ' ', cleanText)\n",
    "    cleanText = re.sub(r'[^\\x00-\\x7f]', ' ', cleanText) \n",
    "    cleanText = re.sub('\\s+', ' ', cleanText)\n",
    "    return cleanText.strip()\n",
    "\n",
    "# Predict the position from resume text\n",
    "def predict_position_from_text(text):\n",
    "    cleaned_text = cleanResume(text)\n",
    "    vectorized = tfidf.transform([cleaned_text]).toarray()\n",
    "    prediction = svc_model.predict(vectorized)\n",
    "    return le.inverse_transform(prediction)[0]\n",
    "\n",
    "\n",
    "\n",
    "# Resume Scoring\n",
    "def calculate_resume_score(info):\n",
    "    score = 0\n",
    "    score += 25 if info['first_name'] and info['last_name'] else 0\n",
    "    score += 25 if info['email'] else 0\n",
    "    score += 25 if info['degree_major'] else 0\n",
    "    score += 25 if info['skills'] else 0\n",
    "    return score\n",
    "\n",
    "\n",
    "\n",
    "# Suggested Skills for Job Title\n",
    "def suggest_skills_for_job(job):\n",
    "    job_skills = {}\n",
    "    with open('sugestedSkills.csv') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            job_title = row[0].strip().lower()\n",
    "            skills = [s.strip() for s in row[1:] if s]\n",
    "            job_skills[job_title] = skills\n",
    "    return job_skills.get(job.lower(), [])\n",
    "\n",
    "\n",
    "\n",
    "## testing\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "# Load resume PDF and extract text\n",
    "doc = fitz.open(r\"C:\\Users\\HP\\Desktop\\sample_of_a_resume5.pdf\")\n",
    "text = \"\\n\".join([page.get_text() for page in doc])\n",
    "\n",
    "# Extract all info\n",
    "first_name, last_name = extract_name(text)\n",
    "email = extract_email(text)\n",
    "phone = extract_contact_number(text)\n",
    "skills = extract_skills(text)\n",
    "degree_major = extract_major(text)\n",
    "experience = extract_experience(text)\n",
    "\n",
    "# Use ML model to predict position\n",
    "predicted_position = predict_position_from_text(text)\n",
    "\n",
    "resume_info = {\n",
    "    'first_name': first_name,\n",
    "    'last_name': last_name,\n",
    "    'email': email,\n",
    "    'phone': phone,\n",
    "    'skills': skills,\n",
    "    'degree_major': degree_major,\n",
    "    'experience': {\n",
    "        'level_of_experience': experience['level_of_experience'],\n",
    "        'suggested_position': predicted_position   # REPLACED OLD POSITION WITH ML ONE\n",
    "    }\n",
    "}\n",
    "\n",
    "# Display\n",
    "print(\"First Name:\", resume_info['first_name'])\n",
    "print(\"Last Name:\", resume_info['last_name'])\n",
    "print(\"Email:\", resume_info['email'])\n",
    "print(\"Phone:\", resume_info['phone'])\n",
    "print(\"Degree/Major:\", resume_info['degree_major'])\n",
    "print(\"Skills:\", ', '.join(resume_info['skills']))\n",
    "print(\"Experience Level:\", resume_info['experience']['level_of_experience'])\n",
    "print(\"Suggested Position:\", resume_info['experience']['suggested_position'])\n",
    "\n",
    "score = calculate_resume_score(resume_info)\n",
    "print(\"Resume Score:\", score)\n",
    "\n",
    "suggested_skills = suggest_skills_for_job(resume_info['experience']['suggested_position'])\n",
    "print(\"Suggested Skills for This Role:\", ', '.join(suggested_skills) if suggested_skills else \"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2850124f-e09c-4a89-a8c6-b0a5f4c7da1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
